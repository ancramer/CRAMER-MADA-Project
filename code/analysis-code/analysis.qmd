---
title: "Analysis"
---

To start, I'll load packages. As I need packages along the way, I'll return here and add them.
```{r}
library(tidyverse)
library(survey)
library(tableone)
library(mice)
library(mitools)
library(miceadds)
library(tidymodels)
library(caret)
library(gt)
library(scales)
library(pROC)
library(ranger)
library(glmnet)
library(Metrics)
library(purrr)
library(here)
```

Now, loading data using the here package.
```{r}
here() #set working directory
#data <- readRDS(here("data","processed-data", "data3.rds"))
#data <- labelled::remove_val_labels(data) #removes labels from haven package which cause problems

data_imputed <- readRDS(here("data","processed-data", "binary_imputed_datasets.rds"))
```

As a reminder, we are going to use the following variables, and stratify by covid test result.
EXERCISE: Whether or not individual met guidelines by Am Heart Assoc met for aerobics, strength, or both, 0 if not met, 1 if any were met
SLEEP: If individual reported getting the proper amount of sleep for their resspective age group
DEPRESSED: If individual felt depressed weekly+ or not
TROUBLE_SLEEPING: If individual reported having trouble sleeping over past several days, more than half days, or nearly every day or not at all
SOCIAL: If person felt like they were reciveing less social support or not
COVID: If they've tested positive for covid or not

We create estimates and CIs using complex survery design variables, using the multiple imputed data. This will find estimates for overall, and by COVID-19 test status.
```{r}
# Create survey design objects for each transformed dataset
svy_designs <- lapply(data_imputed, function(data) {
  svydesign(ids = ~PSU, strata = ~STRATA, weights = ~SAMPWEIGHT, data = data , nest=TRUE)
})


get_complete_stratified_estimates <- function(var_name) {
  results <- data.frame(
    Variable = character(),
    COVID_Status = character(),
    Binary_Value = numeric(),
    Estimate = numeric(),
    CI_lower = numeric(),
    CI_upper = numeric(),
    stringsAsFactors = FALSE
  )

  # Loop through COVID status: Negative (0), Positive (1), Overall (NA)
  for (covid_val in c(0, 1, NA)) {
    if (is.na(covid_val)) {
      covid_label <- "Overall"
      subset_designs <- svy_designs  # No subsetting
    } else {
      covid_label <- ifelse(covid_val == 0, "Negative", "Positive")
      subset_designs <- lapply(svy_designs, function(design) subset(design, COVID == covid_val))
    }

    # Calculate proportions for this COVID status or overall
    props <- lapply(subset_designs, function(design) {
      formula <- as.formula(paste0("~factor(", var_name, ")"))
      svymean(formula, design, na.rm = TRUE)
    })

    # Pool results
    pooled <- MIcombine(props)
    coef_names <- names(coef(pooled))

    for (bin_val in c(0, 1)) {
      possible_names <- c(
        paste0("factor(", var_name, ")", bin_val),
        as.character(bin_val)
      )
      coef_idx <- NULL
      for (name in possible_names) {
        if (name %in% coef_names) {
          coef_idx <- which(coef_names == name)
          break
        }
      }

      if (!is.null(coef_idx)) {
        est <- coef(pooled)[coef_idx]
        ci <- confint(pooled)[coef_idx, ]

        results <- rbind(results, data.frame(
          Variable = var_name,
          COVID_Status = covid_label,
          Binary_Value = bin_val,
          Estimate = round(est * 100, 1),
          CI_lower = round(ci[1] * 100, 1),
          CI_upper = round(ci[2] * 100, 1)
        ))
      }
    }
  }

  return(results)
}
variables <- c("EXERCISE", "SLEEP", "DEPRESSED", "TROUBLE_SLEEPING", "SOCIAL")

# Get complete stratified + overall estimates for each variable
complete_results <- lapply(variables, get_complete_stratified_estimates)

# Combine results into a single data frame
complete_df <- do.call(rbind, complete_results)

# Reshape for better presentation
formatted_results <- complete_df %>%
  mutate(Binary_Label = ifelse(Binary_Value == 1, "Yes", "No")) %>%
  select(Variable, COVID_Status, Binary_Label, Estimate, CI_lower, CI_upper) %>%
  arrange(Variable, COVID_Status, desc(Binary_Label))

print(formatted_results)
```

Now let's turn that into a nice table using GT, and I'll save it to use in the manuscript later. 
```{r}
#checking
summary(formatted_results)
str(formatted_results)
names(formatted_results)

table1_imputed <- formatted_results %>%
  mutate(Binary_Label = ifelse(Binary_Label == "Yes", 1, 0),
         Variable_Label = paste(Variable, Binary_Label, sep = " = ")) %>%
  pivot_wider(
    names_from = COVID_Status,
    values_from = c(Estimate, CI_lower, CI_upper),
    names_glue = "{COVID_Status}_{.value}"
  ) %>%
  mutate(
    Estimate_Negative = Negative_Estimate * 0.01,
    CI_Negative = sprintf("(%.2f, %.2f)", Negative_CI_lower * 0.01, Negative_CI_upper * 0.01),
    Estimate_Positive = Positive_Estimate * 0.01,
    CI_Positive = sprintf("(%.2f, %.2f)", Positive_CI_lower * 0.01, Positive_CI_upper * 0.01),
    Estimate_Overall = Overall_Estimate * 0.01,
    CI_Overall = sprintf("(%.2f, %.2f)", Overall_CI_lower * 0.01, Overall_CI_upper * 0.01)
  ) %>%
  select(Variable_Label, Estimate_Negative, CI_Negative,
         Estimate_Positive, CI_Positive, Estimate_Overall, CI_Overall) %>%
  gt() %>%
  tab_header(
    title = "Estimated proportions of mental health related factors by COVID test status"
  ) %>%
  tab_spanner(
    label = "COVID Negative",
    columns = c(Estimate_Negative, CI_Negative)
  ) %>%
  tab_spanner(
    label = "COVID Positive",
    columns = c(Estimate_Positive, CI_Positive)
  ) %>%
  tab_spanner(
    label = "Overall",
    columns = c(Estimate_Overall, CI_Overall)
  ) %>%
  cols_label(
    Variable_Label = "Variable",
    Estimate_Negative = "Estimate",
    CI_Negative = "95% CI",
    Estimate_Positive = "Estimate",
    CI_Positive = "95% CI",
    Estimate_Overall = "Estimate",
    CI_Overall = "95% CI"
  )

print(table1_imputed)

# Save the table as a PNG
here()
gtsave(table1_imputed, here("results", "figures", "table1_imputed.png"))
```
################################################################################
Now, we will start modeling COVID postivity as an outcome of mental health factors. 

Up first, we will create singleton models to individually study the effects of mental health factors on covid test positivity status.

```{r}
# Create a survey design object for each completed dataset
survey_designs <- lapply(data_imputed, function(data_i) {
  svydesign(
    id = ~PSU,
    strata = ~STRATA,
    weights = ~SAMPWEIGHT,
    nest = TRUE,
    data = data_i
  )
})

# Fit svyglm models for EXERCISE predictor (and other predictors if needed)
predictors <- c("EXERCISE", "DEPRESSED", "SLEEP", "TROUBLE_SLEEPING", "SOCIAL")

# Fit models for each predictor
models <- lapply(predictors, function(predictor) {
  lapply(survey_designs, function(dsgn) {
    formula <- as.formula(paste("COVID ~", predictor))
    svyglm(formula, design = dsgn, family = quasibinomial())
  })
})

# Extract coefficients and standard errors for each model
model_coef <- mapply(function(predictor, predictor_models) {
  lapply(predictor_models, function(model) {
    data.frame(
      Estimate = coef(model),
      Std.Error = sqrt(diag(vcov(model))),
      stringsAsFactors = FALSE
    )
  })
}, predictors, models, SIMPLIFY = FALSE)

# Calculate means, variances, and p-values for pooling
pooled_results <- lapply(names(model_coef), function(predictor_name) {
  coefs <- model_coef[[predictor_name]]
  
  estimates <- do.call(rbind, lapply(coefs, function(x) x$Estimate))
  se <- do.call(rbind, lapply(coefs, function(x) x$Std.Error))
  
  # Compute the mean across imputations
  mean_estimate <- apply(estimates, 2, mean)
  
  # Compute within-imputation variance (average variance across imputations)
  within_variance <- apply(se^2, 2, mean)
  
  # Compute between-imputation variance (variance of the estimates across imputations)
  between_variance <- apply(estimates, 2, var)
  
  # Calculate the total variance using Rubin's rule
  total_variance <- within_variance + (1 + 1 / length(survey_designs)) * between_variance
  
  # Calculate the pooled standard error
  pooled_se <- sqrt(total_variance)
  
  # Calculate the 95% confidence intervals
  ci_low <- mean_estimate - 1.96 * pooled_se
  ci_high <- mean_estimate + 1.96 * pooled_se
  
  # Calculate z-statistics and p-values
  z_stat <- mean_estimate / pooled_se
  p_values <- 2 * (1 - pnorm(abs(z_stat)))  # Two-tailed p-value
  
  # Exponentiate to get Odds Ratios (ORs) from log-odds
  or_estimate <- exp(mean_estimate)
  ci_low_or <- exp(ci_low)
  ci_high_or <- exp(ci_high)
  
  # Create a data frame with pooled Odds Ratios, standard errors, confidence intervals, and p-values
  data.frame(
    Predictor = rep(predictor_name, length(mean_estimate)),
    OR = or_estimate,
    Std.Error = pooled_se,
    CI_Low = ci_low_or,
    CI_High = ci_high_or,
    p_value = p_values,
    row.names = names(mean_estimate)
  )
})

# Combine results for all predictors into one data frame
pooled_exercise <- do.call(rbind, pooled_results)

# Display the labeled pooled results
pooled_exercise

```

Now we will make a nice table of that using gt and save it to use in the mauscript.

```{r}
names(pooled_exercise)
str(pooled_exercise)

#not including intercepts values
table2 <- pooled_exercise %>%
  group_by(Predictor) %>%
  slice(2) %>%  # Select only the second row for each Predictor
  ungroup() %>%
  mutate(CI = paste0("(", round(CI_Low, 3), ", ", round(CI_High, 3), ")")) %>%
  select(Predictor, OR, CI, p_value) %>%
  gt() %>%
  tab_header(
    title = "Associations of individual mental health factors with positive covid test result"
  ) %>%
  cols_label(
    Predictor = "Predictor",
    OR = "Odds Ratio",
    CI = "Confidence Interval",
    p_value = "P-Value"
  ) %>%
  fmt_number(
    columns = vars(OR, p_value),
    decimals = 3
  )

# Save the table as a PNG
here()
gtsave(table2, here("results", "figures", "single_model_table.png"))

```

################################################################################
This is the part of the project in which I demonstrate complex techniques learned during this class.

Next, we will make the multivariate model. I will use cross validation for this. Pooled AUC is calculated and a observed-expected plot is generated.
```{r}
cross_validation_imputed_auc_logit <- function(imputed_data, predictors, k_folds = 5) {
  auc_list <- numeric(length(imputed_data))  # Store AUC for each imputation

  for (i in seq_along(imputed_data)) {
    data <- imputed_data[[i]]

    # Drop rows with missing values in outcome, predictors, or weights
    data <- data %>% drop_na(all_of(c("COVID", predictors, "SAMPWEIGHT")))

    set.seed(123)
    folds <- sample(1:k_folds, size = nrow(data), replace = TRUE)

    fold_auc <- numeric(k_folds)

    for (fold in 1:k_folds) {
      train_data <- data[folds != fold, ]
      test_data  <- data[folds == fold, ]

      # Fit logistic regression model
      formula <- as.formula(paste("COVID ~", paste(predictors, collapse = " + ")))
      logit_model <- glm(
        formula = formula,
        data = train_data,
        family = binomial(),
        weights = SAMPWEIGHT
      )

      # Predict probabilities on test data
      pred_probs <- predict(logit_model, newdata = test_data, type = "response")
      actual <- test_data$COVID

      # Compute AUC for this fold
      auc_result <- roc(actual, pred_probs)$auc
      fold_auc[fold] <- auc_result
    }

    auc_list[i] <- mean(fold_auc)
  }

  overall_auc <- mean(auc_list)

  return(list(auc_per_imputation = auc_list, overall_auc = overall_auc))
}

# Usage
results_logit <- cross_validation_imputed_auc_logit(
  imputed_data = data_imputed,
  predictors = c("EXERCISE", "SLEEP", "DEPRESSED", "TROUBLE_SLEEPING", "SOCIAL")
)

# View results
results_logit$auc_per_imputation
results_logit$overall_auc

# Function to compute calibration data per imputation
get_calibration_data <- function(data, predictors, n_bins = 10) {
  # Fit logistic regression model
  formula <- as.formula(paste("COVID ~", paste(predictors, collapse = " + ")))
  model <- glm(formula, data = data, family = binomial(), weights = SAMPWEIGHT)
  
  # Predict probabilities
  data$predicted <- predict(model, type = "response")
  
  # Bin predictions into deciles
  data <- data %>%
    mutate(bin = ntile(predicted, n_bins)) %>%
    group_by(bin) %>%
    summarise(
      mean_pred = mean(predicted),
      obs_rate = mean(COVID),
      .groups = "drop"
    )
  
  return(data)
}

# Combine calibration data across imputations
all_calib_data <- purrr::map_dfr(
  .x = seq_along(data_imputed),
  .f = function(i) {
    data <- data_imputed[[i]] %>% 
      drop_na(COVID, EXERCISE, SLEEP, DEPRESSED, TROUBLE_SLEEPING, SOCIAL, SAMPWEIGHT)
    
    calib <- get_calibration_data(data, predictors = c("EXERCISE", "SLEEP", "DEPRESSED", "TROUBLE_SLEEPING", "SOCIAL"))
    calib$imputation <- paste0("Imputation ", i)
    return(calib)
  }
)

# Plot calibration curves
LogReg_plot <- ggplot(all_calib_data, aes(x = mean_pred, y = obs_rate, color = imputation)) +
  geom_line() +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray40") +
  scale_x_continuous("Expected Probability", limits = c(0, 1), labels = percent_format(accuracy = 1)) +
  scale_y_continuous("Observed Proportion", limits = c(0, 1), labels = percent_format(accuracy = 1)) +
  labs(title = "Calibration Plot: Logistic Regression (All 5 Imputations)", color = "Imputation") +
  theme_minimal()

# Save the plot using the `here` package
ggsave(filename = here::here("results", "figures", "log_calibration_plot.png"),
       plot = LogReg_plot,
       width = 8, height = 6, dpi = 300)

```

Now, we will make a LASSO regression model. For this, I'm going to use the multiple imputation dataset with cross validation with 5 folds. Instead of using complex survey design, I will weight the model by sampleweight. Pooled AUC is calculated and a observed-expected plot is generated.

```{r}
set.seed(1234)
k <- 5
auc_list <- c()

# Loop through 5 imputed datasets
for (i in 1:5) {
  data1 <- data_imputed[[i]]

  # Remove rows with missing outcome
  data1_complete <- data1 %>% filter(!is.na(COVID))

  # Create folds
  folds <- createFolds(data1_complete$COVID, k = k, list = TRUE, returnTrain = FALSE)

  preds <- rep(NA, nrow(data1_complete))

  for (fold_idx in seq_along(folds)) {
    test_idx <- folds[[fold_idx]]
    train_idx <- setdiff(seq_len(nrow(data1_complete)), test_idx)

    train_data <- data1_complete[train_idx, ] %>% drop_na()
    test_data  <- data1_complete[test_idx, ] %>% drop_na()

    # Create model matrix (remove outcome column)
    x_train <- model.matrix(COVID ~ ., data = train_data)[, -1]
    y_train <- train_data$COVID

    x_test <- model.matrix(COVID ~ ., data = test_data)[, -1]

    # Fit Lasso with CV
    lasso_fit <- cv.glmnet(x = x_train, y = y_train, alpha = 1, family = "binomial")

    # Predict probabilities (type = "response" for prob)
    preds[test_idx] <- predict(lasso_fit, newx = x_test, s = "lambda.min", type = "response")
  }

  actuals <- data1_complete$COVID

  # AUC using pROC
  roc_obj <- roc(actuals, preds)
  fold_auc <- auc(roc_obj)
  auc_list[i] <- fold_auc
}

auc_list
pooled_auc <- mean(auc_list)
print(pooled_auc)

################################################################################
## plot expected vs observed

set.seed(1234)
k <- 5
all_preds_df <- data.frame()

for (i in 1:5) {
  data1 <- data_imputed[[i]]
  data1_complete <- data1 %>% filter(!is.na(COVID))

  folds <- createFolds(data1_complete$COVID, k = k, list = TRUE, returnTrain = FALSE)

  preds <- rep(NA, nrow(data1_complete))

  for (fold_idx in seq_along(folds)) {
    test_idx <- folds[[fold_idx]]
    train_idx <- setdiff(seq_len(nrow(data1_complete)), test_idx)

    train_data <- data1_complete[train_idx, ] %>% drop_na()
    test_data  <- data1_complete[test_idx, ] %>% drop_na()

    x_train <- model.matrix(COVID ~ ., data = train_data)[, -1]
    y_train <- train_data$COVID
    x_test  <- model.matrix(COVID ~ ., data = test_data)[, -1]

    lasso_fit <- cv.glmnet(x = x_train, y = y_train, alpha = 1, family = "binomial")

    preds[test_idx] <- predict(lasso_fit, newx = x_test, s = "lambda.min", type = "response")
  }

  all_preds_df <- bind_rows(all_preds_df, data.frame(
    imputation = i,
    predicted = preds,
    observed = data1_complete$COVID
  ))
}

# Bin predictions into 10 bins and compute calibration stats
calib_data <- all_preds_df %>%
  mutate(bin = ntile(predicted, 10)) %>%
  group_by(imputation, bin) %>%
  summarise(
    mean_pred = mean(predicted, na.rm = TRUE),
    obs_rate = mean(observed, na.rm = TRUE),
    .groups = "drop"
  )

Lasso_plot <- ggplot(calib_data, aes(x = mean_pred, y = obs_rate, color = factor(imputation))) +
  geom_line() +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray40") +
  scale_x_continuous("Expected Probability", limits = c(0, 1), labels = percent_format(accuracy = 1)) +
  scale_y_continuous("Observed Proportion", limits = c(0, 1), labels = percent_format(accuracy = 1)) +
  labs(
    title = "Calibration Plot: Lasso Logistic Regression (All 5 Imputations)",
    color = "Imputation"
  ) +
  theme_minimal()

print(Lasso_plot)

# Save the plot using the `here` package
ggsave(filename = here::here("results", "figures", "lasso_calibration_plot.png"),
       plot = Lasso_plot,
       width = 8, height = 6, dpi = 300)
```

Finally, we will construct a random forest model. For this, I will not use the multiple imputation dataset, as before, I will use the first imputed dataset, with cross validation 5 folds. Pooled AUC is calculated and a observed-expected plot is generated.

```{r}
cross_validation_imputed_predictions <- function(imputed_data, predictors, k_folds = 5) {
  auc_list <- numeric(length(imputed_data))  # Store AUCs
  predictions_list <- vector("list", length(imputed_data))  # Store predictions and actuals

  for (i in 1:length(imputed_data)) {
    data <- imputed_data[[i]] %>%
      drop_na(all_of(c("COVID", predictors, "SAMPWEIGHT")))
    
    set.seed(123)
    folds <- sample(1:k_folds, size = nrow(data), replace = TRUE)
    
    fold_auc <- numeric(k_folds)
    imputation_preds <- tibble()  # Empty tibble to collect preds for this imputation

    for (fold in 1:k_folds) {
      train_data <- data[folds != fold, ]
      test_data  <- data[folds == fold, ]

      formula <- as.formula(paste("COVID ~", paste(predictors, collapse = " + ")))
      rf_model <- ranger(
        formula = formula,
        data = train_data,
        case.weights = train_data$SAMPWEIGHT,
        probability = TRUE
      )

      pred_probs <- predict(rf_model, data = test_data)$predictions[, 2]
      actual <- test_data$COVID

      # Store predictions
      imputation_preds <- bind_rows(imputation_preds, tibble(actual = actual, predicted = pred_probs))

      # AUC
      auc_result <- roc(actual, pred_probs)$auc
      fold_auc[fold] <- auc_result
    }

    auc_list[i] <- mean(fold_auc)
    predictions_list[[i]] <- imputation_preds
  }

  list(
    auc_per_imputation = auc_list,
    overall_auc = mean(auc_list),
    predictions = predictions_list  # List of tibbles with actual + predicted
  )
}
# Assuming you have your imputed_data and a vector of predictors
results_with_preds2 <- cross_validation_imputed_predictions(
  imputed_data = data_imputed,
  predictors = c("EXERCISE", "SLEEP", "DEPRESSED", "TROUBLE_SLEEPING", "SOCIAL")
)

# View results
results_with_preds2$auc_per_imputation  # AUC for each imputed dataset
results_with_preds2$overall_auc         # Average AUC across imputations

# Create calibration plot from predictions across imputations
plot_calibration_multiple <- function(predictions_list, n_bins = 10, title = "Calibration Plot") {
  calib_data_all <- map2_dfr(predictions_list, seq_along(predictions_list), function(df, i) {
    df %>%
      mutate(bin = ntile(predicted, n_bins)) %>%
      group_by(bin) %>%
      summarise(
        mean_pred = mean(predicted),
        obs_rate = mean(actual),
        .groups = "drop"
      ) %>%
      mutate(imputation = paste0("Imputation ", i))
  })

  ggplot(calib_data_all, aes(x = mean_pred, y = obs_rate, color = imputation)) +
    geom_line() +
    geom_point(size = 2) +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray") +
    scale_x_continuous("Expected probability", limits = c(0, 1), labels = scales::percent_format(accuracy = 1)) +
    scale_y_continuous("Observed proportion", limits = c(0, 1), labels = scales::percent_format(accuracy = 1)) +
    labs(title = title, color = "Imputation") +
    theme_minimal()
}

# Plot it
plot_cal <- plot_calibration_multiple(results_with_preds$predictions, title = "Calibration Plot: RF across 5 Imputations")
print(plot_cal)

# Save it
ggsave(here("results", "figures", "rf_calibration_all_imputations.png"), plot_cal, width = 8, height = 6, dpi = 300)

```

Let's put these into a nice table for the manuscript. 
```{r}
# Create a data frame with the results
auc_table <- data.frame(
  Model = c("Multivariate", "Lasso", "Random Forest"),
  AUC = c(0.523, 0.529, 0.556)
)

# Create and style the GT table
model_table <- auc_table %>%
  gt() %>%
  tab_header(
    title = "Model Performance Comparison",
  ) %>%
  fmt_number(
    columns = vars(AUC),
    decimals = 3
  )

# Print the table
model_table

# Save the table as a PNG
here()
gtsave(model_table, here("results", "figures", "model_table.png"))


# Store your AUC lists here
aauc_multivar <- results$auc_per_imputation  # From multivariate logistic regression with CV
auc_lasso     <- unlist(auc_list)                      # From your LASSO section
auc_rf        <- auc_list                              # From your final RF model section

# Create a data frame with AUC values and model type
auc_df <- data.frame(
  Imputation = rep(1:5, 3),
  AUC = c(auc_multivar, auc_lasso, auc_rf),
  Model = rep(c("Multivariate Logistic Regression", "LASSO", "Random Forest"), each = 5)
)

# Plot: AUCs across imputations by model
aucs <- ggplot(auc_df, aes(x = Model, y = AUC, color = Model)) +
  geom_jitter(width = 0.2, size = 2) +         # dots for individual AUCs
  stat_summary(fun = mean, geom = "point", shape = 18, size = 4, color = "black") +
  stat_summary(fun = mean, geom = "text", aes(label = round(..y.., 3)), 
               vjust = -1, color = "black", fontface = "bold") +
  labs(title = "AUC Across Imputations by Model",
       x = "Model",
       y = "AUC") +
  theme_minimal()

# Save the table as a PNG
here()
ggsave(
  filename = here("results", "figures", "auc_plot.png"),
  plot = aucs,  # replace with your plot object name
  width = 8,
  height = 6,
  dpi = 300
)

```


Now, we will make a plot of the AUCs.
```{r}
# Step 1: Combine AUCs into a tidy data frame
auc_df <- data.frame(
  model = rep(c("Logistic Regression", "Lasso", "Random Forest"), each = 5),
  imputation = rep(1:5, times = 3),
  AUC = c(
    results_logit$auc_per_imputation,
    auc_list,
    results_with_preds2$auc_per_imputation
  )
)

# Step 2: Calculate mean AUC per model
auc_means <- auc_df %>%
  group_by(model) %>%
  summarise(mean_auc = mean(AUC), .groups = "drop")

# Step 3: Plot
ggplot(auc_df, aes(x = model, y = AUC, color = model)) +
  geom_jitter(width = 0.15, size = 3, alpha = 0.8) +  # dots for each imputation
  geom_point(data = auc_means, aes(x = model, y = mean_auc),
             color = "black", size = 4) +             # black dot for mean
  geom_text(data = auc_means, aes(x = model, y = mean_auc, 
             label = formatC(mean_auc, format = "f", digits = 4)),
            vjust = -1.2, color = "black", size = 4.5) +  # label for mean
  labs(
    title = "AUCs by Model Type Across 5 Imputations",
    x = "Model",
    y = "AUC"
  ) +
  theme_minimal(base_size = 14) +
  scale_y_continuous(limits = c(0.45, 0.6), breaks = seq(0.45, 0.6, 0.05)) +
  scale_color_brewer(palette = "Dark2") +
  theme(legend.position = "none")

#save plot
ggsave(
  filename = here("results", "figures", "auc_comparison_plot.png"),  # save in outputs/ directory
  plot = last_plot(),       # or specify your plot object explicitly
  width = 8, height = 6,    # size in inches
  dpi = 300                 # high-quality resolution
)
```
