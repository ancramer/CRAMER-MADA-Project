---
title: "Analysis"
---

To start, I'll load packages. As I need packages along the way, I'll return here and add them.
```{r}
library(tidyverse)
library(survey)
library(tableone)
library(mice)
library(mitools)
library(miceadds)
library(tidymodels)
library(caret)
library(gt)
library(pROC)
library(ranger)
library(glmnet)
library(Metrics)
library(purrr)
library(here)
```

Now, loading data using the here package.
```{r}
here() #set working directory
#data <- readRDS(here("data","processed-data", "data3.rds"))
#data <- labelled::remove_val_labels(data) #removes labels from haven package which cause problems

data_imputed <- readRDS(here("data","processed-data", "binary_imputed_datasets.rds"))
```

As a reminder, we are going to use the following variables, and stratify by covid test result.
EXERCISE: Whether or not individual met guidelines by Am Heart Assoc met for aerobics, strength, or both, 0 if not met, 1 if any were met
SLEEP: If individual reported getting the proper amount of sleep for their resspective age group
DEPRESSED: If individual felt depressed weekly+ or not
TROUBLE_SLEEPING: If individual reported having trouble sleeping over past several days, more than half days, or nearly every day or not at all
SOCIAL: If person felt like they were reciveing less social support or not
COVID: If they've tested positive for covid or not

We create estimates and CIs using complex survery design variables, using the multiple imputed data.

```{r}

# Create survey design objects for each transformed dataset
svy_designs <- lapply(data_imputed, function(data) {
  svydesign(ids = ~PSU, strata = ~STRATA, weights = ~SAMPWEIGHT, data = data , nest=TRUE)
})

# Function to get complete estimates (both 0 and 1) stratified by COVID status
get_complete_stratified_estimates <- function(var_name) {
  results <- data.frame(
    Variable = character(),
    COVID_Status = character(),
    Binary_Value = numeric(),
    Estimate = numeric(),
    CI_lower = numeric(),
    CI_upper = numeric(),
    stringsAsFactors = FALSE
  )
  
  # Loop through COVID status (0 and 1)
  for (covid_val in c(0, 1)) {
    covid_label <- ifelse(covid_val == 0, "Negative", "Positive")
    
    # Calculate proportions for this COVID status
    props <- lapply(svy_designs, function(design) {
      # Create a subset design
      subdesign <- subset(design, COVID == covid_val)
      # Create formula for the variable
      formula <- as.formula(paste0("~factor(", var_name, ")"))
      # Calculate proportions
      svymean(formula, subdesign, na.rm = TRUE)
    })
    
    # Pool results
    pooled <- MIcombine(props)
    
    # Extract results for both binary values (if available)
    coef_names <- names(coef(pooled))
    
    # Handle different potential naming patterns
    for (bin_val in c(0, 1)) {
      # Check different possible coefficient names
      possible_names <- c(
        paste0("factor(", var_name, ")", bin_val),
        as.character(bin_val)
      )
      
      # Find the matching coefficient name
      coef_idx <- NULL
      for (name in possible_names) {
        if (name %in% coef_names) {
          coef_idx <- which(coef_names == name)
          break
        }
      }
      
      # If found, extract estimates
      if (!is.null(coef_idx)) {
        est <- coef(pooled)[coef_idx]
        ci <- confint(pooled)[coef_idx, ]
        
        # Add to results
        results <- rbind(results, data.frame(
          Variable = var_name,
          COVID_Status = covid_label,
          Binary_Value = bin_val,
          Estimate = round(est * 100, 1),
          CI_lower = round(ci[1] * 100, 1),
          CI_upper = round(ci[2] * 100, 1)
        ))
      }
    }
  }
  
  return(results)
}

# Variables to analyze
variables <- c("EXERCISE", "SLEEP", "DEPRESSED", "TROUBLE_SLEEPING", "SOCIAL")

# Get complete stratified estimates for each variable
complete_results <- lapply(variables, get_complete_stratified_estimates)

# Combine results into a single data frame
complete_df <- do.call(rbind, complete_results)

# Reshape for better presentation
formatted_results <- complete_df %>%
  mutate(Binary_Label = ifelse(Binary_Value == 1, "Yes", "No")) %>%
  select(Variable, COVID_Status, Binary_Label, Estimate, CI_lower, CI_upper) %>%
  arrange(Variable, COVID_Status, desc(Binary_Label))

print(formatted_results)

```

Now let's turn that into a nice table using GT, and I'll save it to use in the manuscript later. 
```{r}
#checking
summary(formatted_results)
str(formatted_results)
names(formatted_results)

# Prepare the data by combining Variable and Binary_Label, pivoting, and formatting the results
table1_imputed <- formatted_results %>%
  mutate(Binary_Label = ifelse(Binary_Label == "Yes", 1, 0),  # Convert Yes to 1 and No to 0
         Variable_Label = paste(Variable, Binary_Label, sep = " = ")) %>%
  pivot_wider(
    names_from = COVID_Status,
    values_from = c(Estimate, CI_lower, CI_upper),
    names_glue = "{COVID_Status}_{.value}"
  ) %>%
  mutate(
    # Scale the estimates and CI by 0.01
    Estimate_Negative = Negative_Estimate * 0.01,
    CI_Negative = sprintf("(%.2f, %.2f)", Negative_CI_lower * 0.01, Negative_CI_upper * 0.01),
    Estimate_Positive = Positive_Estimate * 0.01,
    CI_Positive = sprintf("(%.2f, %.2f)", Positive_CI_lower * 0.01, Positive_CI_upper * 0.01)
  ) %>%
  select(Variable_Label, Estimate_Negative, CI_Negative, Estimate_Positive, CI_Positive) %>%
  gt() %>%
  tab_header(
    title = "Estimated proportions of mental health related factors by COVID test status"
  ) %>%
  tab_spanner(
    label = "COVID Negative",
    columns = c(Estimate_Negative, CI_Negative)
  ) %>%
  tab_spanner(
    label = "COVID Positive",
    columns = c(Estimate_Positive, CI_Positive)
  ) %>%
  cols_label(
    Variable_Label = "Variable",               # Rename Variable_Label to Variable
    Estimate_Negative = "Estimate",            # Rename Estimate_Negative to Estimate
    CI_Negative = "95% CI",                    # Rename CI_Negative to 95% CI
    Estimate_Positive = "Estimate",            # Rename Estimate_Positive to Estimate
    CI_Positive = "95% CI"                     # Rename CI_Positive to 95% CI
  )

# Save the table as a PNG
here()
gtsave(table1_imputed, here("results", "figures", "table1_imputed.png"))
```
################################################################################
Now, we will start modeling COVID postivity as an outcome of mental health factors. 

Up first, we will create singleton models to individually study the effects of mental health factors on covid test positivity status.

```{r}
# Create a survey design object for each completed dataset
survey_designs <- lapply(data_imputed, function(data_i) {
  svydesign(
    id = ~PSU,
    strata = ~STRATA,
    weights = ~SAMPWEIGHT,
    nest = TRUE,
    data = data_i
  )
})

# Fit svyglm models for EXERCISE predictor (and other predictors if needed)
predictors <- c("EXERCISE", "DEPRESSED", "SLEEP", "TROUBLE_SLEEPING", "SOCIAL")

# Fit models for each predictor
models <- lapply(predictors, function(predictor) {
  lapply(survey_designs, function(dsgn) {
    formula <- as.formula(paste("COVID ~", predictor))
    svyglm(formula, design = dsgn, family = quasibinomial())
  })
})

# Extract coefficients and standard errors for each model
model_coef <- mapply(function(predictor, predictor_models) {
  lapply(predictor_models, function(model) {
    data.frame(
      Estimate = coef(model),
      Std.Error = sqrt(diag(vcov(model))),
      stringsAsFactors = FALSE
    )
  })
}, predictors, models, SIMPLIFY = FALSE)

# Calculate means, variances, and p-values for pooling
pooled_results <- lapply(names(model_coef), function(predictor_name) {
  coefs <- model_coef[[predictor_name]]
  
  estimates <- do.call(rbind, lapply(coefs, function(x) x$Estimate))
  se <- do.call(rbind, lapply(coefs, function(x) x$Std.Error))
  
  # Compute the mean across imputations
  mean_estimate <- apply(estimates, 2, mean)
  
  # Compute within-imputation variance (average variance across imputations)
  within_variance <- apply(se^2, 2, mean)
  
  # Compute between-imputation variance (variance of the estimates across imputations)
  between_variance <- apply(estimates, 2, var)
  
  # Calculate the total variance using Rubin's rule
  total_variance <- within_variance + (1 + 1 / length(survey_designs)) * between_variance
  
  # Calculate the pooled standard error
  pooled_se <- sqrt(total_variance)
  
  # Calculate the 95% confidence intervals
  ci_low <- mean_estimate - 1.96 * pooled_se
  ci_high <- mean_estimate + 1.96 * pooled_se
  
  # Calculate z-statistics and p-values
  z_stat <- mean_estimate / pooled_se
  p_values <- 2 * (1 - pnorm(abs(z_stat)))  # Two-tailed p-value
  
  # Exponentiate to get Odds Ratios (ORs) from log-odds
  or_estimate <- exp(mean_estimate)
  ci_low_or <- exp(ci_low)
  ci_high_or <- exp(ci_high)
  
  # Create a data frame with pooled Odds Ratios, standard errors, confidence intervals, and p-values
  data.frame(
    Predictor = rep(predictor_name, length(mean_estimate)),
    OR = or_estimate,
    Std.Error = pooled_se,
    CI_Low = ci_low_or,
    CI_High = ci_high_or,
    p_value = p_values,
    row.names = names(mean_estimate)
  )
})

# Combine results for all predictors into one data frame
pooled_exercise <- do.call(rbind, pooled_results)

# Display the labeled pooled results
pooled_exercise

```

Now we will make a nice table of that using gt and save it to use in the mauscript.

```{r}
names(pooled_exercise)
str(pooled_exercise)

#not including intercepts values
table2 <- pooled_exercise %>%
  group_by(Predictor) %>%
  slice(2) %>%  # Select only the second row for each Predictor
  ungroup() %>%
  mutate(CI = paste0("(", round(CI_Low, 3), ", ", round(CI_High, 3), ")")) %>%
  select(Predictor, OR, CI, p_value) %>%
  gt() %>%
  tab_header(
    title = "Associations of individual mental health factors with positive covid test result"
  ) %>%
  cols_label(
    Predictor = "Predictor",
    OR = "Odds Ratio",
    CI = "Confidence Interval",
    p_value = "P-Value"
  ) %>%
  fmt_number(
    columns = vars(OR, p_value),
    decimals = 3
  )

# Save the table as a PNG
here()
gtsave(table2, here("results", "figures", "single_model_table.png"))

```

################################################################################
This is the part of the project in which I demonstrate complex techniques learned during this class.

Next, we will make the multivariate model. I will use cross validation for this. The pooled AUC across imputations is found to be 0.5230.
```{r}
# Define the AUC cross-validation function for imputed datasets
cross_validation_imputed_auc <- function(imputed_data, predictors, k_folds = 5) {
  auc_list <- numeric(length(imputed_data))  # Store AUC for each imputation

  for (i in 1:length(imputed_data)) {
    data <- imputed_data[[i]]
    
    # drop rows with missing values in COVID, predictors, or SAMPWEIGHT
    data <- data %>% drop_na(all_of(c("COVID", predictors, "SAMPWEIGHT")))
    
    set.seed(123)
    folds <- sample(1:k_folds, size = nrow(data), replace = TRUE)
    
    fold_auc <- numeric(k_folds)  # AUC for each fold
    
    for (fold in 1:k_folds) {
      train_data <- data[folds != fold, ]
      test_data  <- data[folds == fold, ]
      
      # Fit random forest model on training data
      formula <- as.formula(paste("COVID ~", paste(predictors, collapse = " + ")))
      rf_model <- ranger(
        formula = formula,
        data = train_data,
        case.weights = train_data$SAMPWEIGHT,
        probability = TRUE
      )
      
      # Predict probabilities for class 1 (COVID = 1)
      pred_probs <- predict(rf_model, data = test_data)$predictions[, 2]
      actual <- test_data$COVID
      
      # Compute AUC for this fold
      auc_result <- roc(actual, pred_probs)$auc
      fold_auc[fold] <- auc_result
    }
    
    # Average AUC over folds for this imputed dataset
    auc_list[i] <- mean(fold_auc)
  }
  
  # Average AUC over all imputed datasets
  overall_auc <- mean(auc_list)
  
  return(list(auc_per_imputation = auc_list, overall_auc = overall_auc))
}

# Assuming you have your imputed_data and a vector of predictors
results <- cross_validation_imputed_auc(imputed_data = data_imputed,
                                        predictors = c("EXERCISE", "SLEEP", "DEPRESSED", "TROUBLE_SLEEPING", "SOCIAL"))

# View results
results$auc_per_imputation  # AUC for each imputed dataset
results$overall_auc         # Average AUC across imputations
```

Now, we will make a LASSO regression model. For this, I'm going to use the multiple imputation dataset with cross validation with 5 folds. Instead of using complex survey design, I will weight the model by sampleweight. The pooled AUC is  0.529. 

```{r}
# Extract the first completed imputed dataset
data1 <- data_imputed[[1]]

# Define the survey design
survey_design <- svydesign(
  ids = ~PSU,
  strata = ~STRATA,
  weights = ~SAMPWEIGHT,
  data = data1,
  nest=TRUE
)

set.seed(1234)  # for reproducibility

# Choose outcome and predictors
outcome_var <- "COVID"
predictor_vars <- c("EXERCISE", "SLEEP", "DEPRESSED", "TROUBLE_SLEEPING", "SOCIAL")

# Loop over imputations and compute AUC
auc_list <- map(data_imputed, function(dat) {
  # Drop missing in predictors or outcome if any slipped through
  dat <- dat %>% drop_na(all_of(c(outcome_var, predictor_vars, "SAMPWEIGHT")))

  # Create design matrix X and response y
  X <- model.matrix(as.formula(paste(outcome_var, "~", paste(predictor_vars, collapse = "+"))), dat)[, -1]
  y <- dat[[outcome_var]]
  w <- dat$SAMPWEIGHT

  # Split into training and test sets (e.g., 70/30 split)
  train_idx <- sample(seq_len(nrow(X)), size = 0.7 * nrow(X))
  test_idx <- setdiff(seq_len(nrow(X)), train_idx)

  X_train <- X[train_idx, ]
  y_train <- y[train_idx]
  w_train <- w[train_idx]

  X_test <- X[test_idx, ]
  y_test <- y[test_idx]
  w_test <- w[test_idx]

  # Fit random forest model (instead of lasso, because it's a classification problem)
  rf_model <- ranger(
    formula = as.formula(paste(outcome_var, "~", paste(predictor_vars, collapse = "+"))),
    data = dat[train_idx, ],
    case.weights = w_train,
    probability = TRUE
  )

  # Predict probabilities for class 1
  pred_probs <- predict(rf_model, data = dat[test_idx, ])$predictions[,2]

  # Compute AUC
  auc_result <- roc(y_test, pred_probs)$auc
  return(auc_result)
})

# Pool AUC across imputations
mean_auc <- mean(unlist(auc_list))

# Output
cat("AUC for each imputation:\n")
print(unlist(auc_list))
cat("\nPooled AUC across imputations:", round(mean_auc, 3), "\n")
```

Finally, we will construct a random forest model. For this, I will not use the multiple imputation dataset, as before, I will use the first imputed dataset, with cross validation 5 folds. The pooled AUC is 0.556.

```{r}
library(ranger)
library(caret)
library(pROC)
library(dplyr)

set.seed(1234)
k <- 5
auc_list <- numeric(length(data_imputed))

for (i in seq_along(data_imputed)) {
  data1 <- data_imputed[[i]]

  # Remove missing outcome
  data1_complete <- data1 %>% filter(!is.na(COVID))

  # Make sure outcome is binary (0/1)
  data1_complete$COVID <- as.numeric(data1_complete$COVID)

  # Add weights column if not present
  data1_complete$weights <- data1_complete$SAMPWEIGHT

  # Create folds
  folds <- createFolds(data1_complete$COVID, k = k, list = TRUE, returnTrain = FALSE)

  auc_folds <- numeric(k)

  for (fold_idx in seq_along(folds)) {
    test_idx <- folds[[fold_idx]]
    train_idx <- setdiff(seq_len(nrow(data1_complete)), test_idx)

    train_data <- data1_complete[train_idx, ]
    test_data  <- data1_complete[test_idx, ]

    # Fit random forest classifier
    rf_model <- ranger(
      formula = COVID ~ EXERCISE + SLEEP + DEPRESSED + TROUBLE_SLEEPING + SOCIAL, #formula
      data = train_data,
      case.weights = train_data$weights,
      probability = TRUE
    )

    # Predict probabilities for class 1
    pred_probs <- predict(rf_model, data = test_data)$predictions[,2]

    # Compute AUC
    auc_folds[fold_idx] <- roc(test_data$COVID, pred_probs, quiet = TRUE)$auc
  }

  # Store average AUC for this imputation
  auc_list[i] <- mean(auc_folds)
}

# Pool AUC across imputations
mean_auc <- mean(auc_list)
print(paste("Pooled AUC across imputations:", round(mean_auc, 3)))
```

Let's put these into a nice table for the manuscript. 
```{r}
# Create a data frame with the results
auc_table <- data.frame(
  Model = c("Multivariate", "Lasso", "Random Forest"),
  AUC = c(0.523, 0.529, 0.556)
)

# Create and style the GT table
model_table <- auc_table %>%
  gt() %>%
  tab_header(
    title = "Model Performance Comparison",
  ) %>%
  fmt_number(
    columns = vars(AUC),
    decimals = 3
  )

# Print the table
model_table

# Save the table as a PNG
here()
gtsave(model_table, here("results", "figures", "model_table.png"))
```

